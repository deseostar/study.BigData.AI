{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a,b,c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# tf.placeholder: 계산을 실행할 때 입력값을 받는 변수로 사용합니다.\n",
    "# None 은 크기가 정해지지 않았음을 의미합니다.\n",
    "X = tf.placeholder(tf.float32, [None,3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=(3, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_3:0' shape=(2, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# X 플레이스홀더에 넣을 값 입니다.\n",
    "# 플레이스홀더에서 설정한 것 처럼, 두번째 차원의 요소의 갯수는 3개 입니다.\n",
    "x_data = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "# tf.Variable: 그래프를 계산하면서 최적화 할 변수들입니다. 이 값이 바로 신경망을 좌우하는 값들입니다.\n",
    "# tf.random_normal: 각 변수들의 초기값을 정규분포 랜덤 값으로 초기화합니다.\n",
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 입력값과 변수들을 계산할 수식을 작성합니다.\n",
    "# tf.matmul 처럼 mat* 로 되어 있는 함수로 행렬 계산을 수행합니다.\n",
    "expr = tf.matmul(X, W) + b\n",
    "#2x3 X 3x2 = 2x2 행렬\n",
    "print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-0.256943    0.22406656]\n",
      " [-1.1709627   0.49366012]\n",
      " [-0.4783712  -1.6060655 ]]\n",
      "=== b ===\n",
      "[[ 0.43213013]\n",
      " [-1.2244205 ]]\n",
      "=== expr ===\n",
      "[[ -3.6018522  -3.1746795]\n",
      " [-10.977234   -7.4962463]]\n"
     ]
    }
   ],
   "source": [
    "# 위에서 설정한 Variable 들의 값들을 초기화 하기 위해\n",
    "# 처음에 tf.global_variables_initializer 를 한 번 실행해야 합니다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "# expr 수식에는 X 라는 입력값이 필요합니다.\n",
    "# 따라서 expr 실행시에는 이 변수에 대한 실제 입력값을 다음처럼 넣어줘야합니다.\n",
    "print(sess.run(expr, feed_dict={X: x_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X:0\", dtype=float32)\n",
      "Tensor(\"Y:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# name: 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줍니다.\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", dtype=float32)\n",
      "Tensor(\"Placeholder_2:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# name: 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줍니다.\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63109136]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\deseo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# X 와 Y 의 상관 관계를 분석하기 위한 가설 수식을 작성합니다.\n",
    "# y = W * x + b\n",
    "# W 와 X 가 행렬이 아니므로 tf.matmul 이 아니라 기본 곱셈 기호를 사용했습니다.\n",
    "hypothesis = W * X + b\n",
    "\n",
    "# 손실 함수를 작성합니다.\n",
    "# mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 정합니다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사 하강법 최적화를 수행합니다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# 비용을 최소화 하는 것이 최종 목표\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.668574 [0.6740768] [1.0118626]\n",
      "1 0.200429 [0.57352674] [0.9398594]\n",
      "2 0.12880684 [0.5956247] [0.9224768]\n",
      "3 0.12194657 [0.60405093] [0.8997316]\n",
      "4 0.116145134 [0.61371076] [0.8781649]\n",
      "5 0.11062809 [0.6229814] [0.8570476]\n",
      "6 0.10537317 [0.63204634] [0.8364455]\n",
      "7 0.100367844 [0.64089155] [0.8163379]\n",
      "8 0.09560031 [0.64952433] [0.7967137]\n",
      "9 0.09105923 [0.6579495] [0.77756125]\n",
      "10 0.086733855 [0.66617215] [0.7588692]\n",
      "11 0.08261391 [0.67419714] [0.74062645]\n",
      "12 0.07868967 [0.68202925] [0.7228223]\n",
      "13 0.07495189 [0.68967307] [0.7054462]\n",
      "14 0.07139157 [0.697133] [0.6884877]\n",
      "15 0.06800046 [0.7044138] [0.671937]\n",
      "16 0.0647704 [0.7115195] [0.6557841]\n",
      "17 0.061693754 [0.7184543] [0.6400195]\n",
      "18 0.05876327 [0.7252225] [0.62463385]\n",
      "19 0.055971965 [0.731828] [0.60961807]\n",
      "20 0.053313255 [0.73827463] [0.59496325]\n",
      "21 0.050780803 [0.7445663] [0.58066076]\n",
      "22 0.048368692 [0.7507068] [0.56670207]\n",
      "23 0.046071112 [0.75669956] [0.55307895]\n",
      "24 0.043882728 [0.7625484] [0.53978336]\n",
      "25 0.041798282 [0.7682566] [0.52680737]\n",
      "26 0.039812844 [0.7738275] [0.5141432]\n",
      "27 0.0379217 [0.77926457] [0.5017836]\n",
      "28 0.036120366 [0.7845708] [0.48972106]\n",
      "29 0.034404624 [0.7897496] [0.47794852]\n",
      "30 0.032770403 [0.7948039] [0.46645898]\n",
      "31 0.031213762 [0.7997367] [0.4552456]\n",
      "32 0.029731104 [0.8045509] [0.44430184]\n",
      "33 0.02831885 [0.80924934] [0.4336211]\n",
      "34 0.02697368 [0.81383485] [0.42319715]\n",
      "35 0.025692396 [0.81831014] [0.41302377]\n",
      "36 0.024472022 [0.82267785] [0.40309498]\n",
      "37 0.023309568 [0.82694054] [0.39340484]\n",
      "38 0.022202335 [0.8311007] [0.38394764]\n",
      "39 0.021147713 [0.83516103] [0.37471783]\n",
      "40 0.020143196 [0.8391236] [0.36570984]\n",
      "41 0.019186387 [0.842991] [0.35691845]\n",
      "42 0.018275004 [0.84676534] [0.34833837]\n",
      "43 0.017406916 [0.85044897] [0.33996454]\n",
      "44 0.016580084 [0.85404414] [0.33179206]\n",
      "45 0.015792515 [0.85755277] [0.323816]\n",
      "46 0.015042367 [0.8609771] [0.3160317]\n",
      "47 0.0143278465 [0.86431915] [0.30843452]\n",
      "48 0.013647259 [0.8675808] [0.30101994]\n",
      "49 0.012999006 [0.87076414] [0.29378366]\n",
      "50 0.012381546 [0.87387085] [0.2867213]\n",
      "51 0.011793402 [0.8769029] [0.2798287]\n",
      "52 0.011233204 [0.879862] [0.2731018]\n",
      "53 0.010699637 [0.8827501] [0.26653662]\n",
      "54 0.010191378 [0.8855687] [0.26012924]\n",
      "55 0.009707288 [0.88831955] [0.2538759]\n",
      "56 0.009246173 [0.89100426] [0.2477729]\n",
      "57 0.008806977 [0.8936245] [0.24181662]\n",
      "58 0.008388647 [0.8961817] [0.23600352]\n",
      "59 0.007990177 [0.89867735] [0.23033012]\n",
      "60 0.0076106316 [0.9011131] [0.22479315]\n",
      "61 0.0072491174 [0.90349025] [0.21938929]\n",
      "62 0.006904792 [0.90581036] [0.21411534]\n",
      "63 0.006576806 [0.90807456] [0.20896812]\n",
      "64 0.0062643983 [0.9102844] [0.20394468]\n",
      "65 0.005966842 [0.91244113] [0.19904199]\n",
      "66 0.005683407 [0.91454595] [0.19425714]\n",
      "67 0.0054134433 [0.9166002] [0.18958734]\n",
      "68 0.005156299 [0.9186051] [0.18502977]\n",
      "69 0.0049113682 [0.9205618] [0.1805818]\n",
      "70 0.0046780766 [0.9224714] [0.17624071]\n",
      "71 0.0044558668 [0.9243352] [0.17200403]\n",
      "72 0.0042442125 [0.9261541] [0.16786915]\n",
      "73 0.0040426087 [0.9279293] [0.1638337]\n",
      "74 0.0038505858 [0.92966187] [0.15989526]\n",
      "75 0.003667672 [0.9313527] [0.15605146]\n",
      "76 0.0034934552 [0.93300295] [0.15230009]\n",
      "77 0.0033275133 [0.93461347] [0.14863889]\n",
      "78 0.0031694497 [0.9361853] [0.14506572]\n",
      "79 0.003018902 [0.9377194] [0.14157847]\n",
      "80 0.0028755043 [0.93921655] [0.138175]\n",
      "81 0.0027389117 [0.9406778] [0.1348534]\n",
      "82 0.0026088182 [0.94210386] [0.13161159]\n",
      "83 0.0024848925 [0.94349563] [0.12844773]\n",
      "84 0.0023668627 [0.94485396] [0.12535992]\n",
      "85 0.0022544307 [0.9461796] [0.12234635]\n",
      "86 0.0021473446 [0.9474734] [0.11940522]\n",
      "87 0.0020453474 [0.94873613] [0.11653481]\n",
      "88 0.0019481899 [0.9499685] [0.1137334]\n",
      "89 0.001855649 [0.9511712] [0.11099933]\n",
      "90 0.0017674981 [0.952345] [0.10833098]\n",
      "91 0.001683542 [0.9534906] [0.10572677]\n",
      "92 0.0016035767 [0.95460874] [0.10318519]\n",
      "93 0.0015274057 [0.9556998] [0.10070465]\n",
      "94 0.0014548544 [0.9567648] [0.0982838]\n",
      "95 0.0013857413 [0.95780414] [0.09592111]\n",
      "96 0.0013199231 [0.9588185] [0.09361523]\n",
      "97 0.0012572206 [0.95980847] [0.09136479]\n",
      "98 0.0011974997 [0.9607746] [0.08916843]\n",
      "99 0.0011406192 [0.9617176] [0.0870249]\n",
      "\n",
      "=== Test ===\n",
      "X: 5, Y: [4.8956127]\n",
      "X: 2.5, Y: [2.491319]\n"
     ]
    }
   ],
   "source": [
    "# 세션을 생성하고 초기화합니다.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 100번 수행합니다.\n",
    "    for step in range(100):\n",
    "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "\n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연습문제\n",
    "sess = tf.Session()\n",
    "x_data = [[1,3], [2,5],[3,7]]\n",
    "y_data = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_3:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"Y_8:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_uniform([2,1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# name: 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줍니다.\n",
    "X = tf.placeholder(tf.float32,[None,2], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X 와 Y 의 상관 관계를 분석하기 위한 가설 수식을 작성합니다.\n",
    "# y = W * x + b\n",
    "# W 와 X 가 행렬이 아니므로 tf.matmul 이 아니라 기본 곱셈 기호를 사용했습니다.\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# 손실 함수를 작성합니다.\n",
    "# mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 정합니다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사 하강법 최적화를 수행합니다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# 비용을 최소화 하는 것이 최종 목표\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2152115 [[-0.3328088]\n",
      " [ 0.5241313]] [0.02336613]\n",
      "1 1.0083823 [[-0.34148437]\n",
      " [ 0.50721204]] [0.02379803]\n",
      "2 0.98372144 [[-0.34553248]\n",
      " [ 0.50157803]] [0.02626024]\n",
      "3 0.9796012 [[-0.34802422]\n",
      " [ 0.49973285]] [0.02939853]\n",
      "4 0.9778031 [[-0.34999067]\n",
      " [ 0.49915966]] [0.03275825]\n",
      "5 0.9762731 [[-0.35177806]\n",
      " [ 0.49901336]] [0.03618674]\n",
      "6 0.97477967 [[-0.3535026 ]\n",
      " [ 0.49901032]] [0.03963279]\n",
      "7 0.9732969 [[-0.35520333]\n",
      " [ 0.49905527]] [0.04307921]\n",
      "8 0.97182125 [[-0.3568934 ]\n",
      " [ 0.49911618]] [0.04652023]\n",
      "9 0.9703531 [[-0.35857716]\n",
      " [ 0.49918237]] [0.04995394]\n",
      "10 0.9688918 [[-0.36025614]\n",
      " [ 0.49925023]] [0.05337971]\n",
      "11 0.96743774 [[-0.36193082]\n",
      " [ 0.4993185 ]] [0.05679734]\n",
      "12 0.96599036 [[-0.36360136]\n",
      " [ 0.49938685]] [0.06020677]\n",
      "13 0.96455026 [[-0.36526784]\n",
      " [ 0.4994551 ]] [0.06360801]\n",
      "14 0.96311706 [[-0.3669303]\n",
      " [ 0.4995232]] [0.06700105]\n",
      "15 0.9616905 [[-0.36858878]\n",
      " [ 0.4995911 ]] [0.07038593]\n",
      "16 0.9602711 [[-0.37024325]\n",
      " [ 0.49965888]] [0.07376265]\n",
      "17 0.9588584 [[-0.37189373]\n",
      " [ 0.4997265 ]] [0.07713123]\n",
      "18 0.9574525 [[-0.37354025]\n",
      " [ 0.49979398]] [0.08049171]\n",
      "19 0.95605344 [[-0.3751828 ]\n",
      " [ 0.49986127]] [0.08384409]\n",
      "20 0.95466083 [[-0.3768214 ]\n",
      " [ 0.49992839]] [0.08718839]\n",
      "21 0.953275 [[-0.37845603]\n",
      " [ 0.49999535]] [0.09052464]\n",
      "22 0.951896 [[-0.38008672]\n",
      " [ 0.50006217]] [0.09385286]\n",
      "23 0.9505236 [[-0.3817135]\n",
      " [ 0.5001288]] [0.09717305]\n",
      "24 0.94915783 [[-0.38333637]\n",
      " [ 0.5001953 ]] [0.10048525]\n",
      "25 0.9477986 [[-0.38495532]\n",
      " [ 0.5002616 ]] [0.10378946]\n",
      "26 0.946446 [[-0.38657036]\n",
      " [ 0.50032777]] [0.10708573]\n",
      "27 0.9450997 [[-0.3881815]\n",
      " [ 0.5003938]] [0.11037405]\n",
      "28 0.9437599 [[-0.38978878]\n",
      " [ 0.5004596 ]] [0.11365445]\n",
      "29 0.9424267 [[-0.39139217]\n",
      " [ 0.5005253 ]] [0.11692695]\n",
      "30 0.9411 [[-0.39299172]\n",
      " [ 0.5005908 ]] [0.12019157]\n",
      "31 0.9397795 [[-0.3945874]\n",
      " [ 0.5006562]] [0.12344833]\n",
      "32 0.9384654 [[-0.39617926]\n",
      " [ 0.5007214 ]] [0.12669724]\n",
      "33 0.9371574 [[-0.39776728]\n",
      " [ 0.5007865 ]] [0.12993833]\n",
      "34 0.9358561 [[-0.39935148]\n",
      " [ 0.5008514 ]] [0.13317162]\n",
      "35 0.9345608 [[-0.40093184]\n",
      " [ 0.5009161 ]] [0.13639711]\n",
      "36 0.9332718 [[-0.4025084]\n",
      " [ 0.5009807]] [0.13961482]\n",
      "37 0.931989 [[-0.40408117]\n",
      " [ 0.5010451 ]] [0.1428248]\n",
      "38 0.93071246 [[-0.40565014]\n",
      " [ 0.5011094 ]] [0.14602704]\n",
      "39 0.92944187 [[-0.40721536]\n",
      " [ 0.50117356]] [0.14922157]\n",
      "\n",
      "=== Test ===\n",
      "X: [5,9], Y: [[2.6237068]]\n",
      "X: [2.5,6], Y: [[2.1382246]]\n"
     ]
    }
   ],
   "source": [
    "# 세션을 생성하고 초기화합니다.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 100번 수행합니다.\n",
    "    for step in range(40):\n",
    "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "\n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: [5,9], Y:\", sess.run(hypothesis, feed_dict={X:[[5,9]]}))\n",
    "    print(\"X: [2.5,6], Y:\", sess.run(hypothesis, feed_dict={X: [[2.5,6]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2, 3] 으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정합니다.\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망에 가중치 W과 편향 b을 적용합니다\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듭니다\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망을 최적화하기 위한 비용 함수를 작성합니다.\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용합니다.\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "# https://curt-park.github.io/2018-09-19/loss-cross-entropy/ (cross-entorpy 자세한 설명)\n",
    "# 즉, cross entropy를 최소화하는 것은 KL Divergence를 최소화하는 것과도 같다. \n",
    "# 그럼으로써 p를 근사하는 q의 확률분포가 최대한 p와 같아질 수 있도록 예측모델의 파라미터를 조정하게된다.\n",
    "# 즉 예측값과 실제값 사이의 확률 분포의 차이를 계산\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.2340947\n",
      "20 1.2284311\n",
      "30 1.2228984\n",
      "40 1.2174926\n",
      "50 1.2122103\n",
      "60 1.2070473\n",
      "70 1.2020001\n",
      "80 1.1970651\n",
      "90 1.1922387\n",
      "100 1.1875176\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "#손실 값은 계속 줄어 들고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 1 1 0 0 1]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 66.67\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax 를 이용해 가장 큰 값의 위치를 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0] 1행은 1번 인덱스, 2행은 0번 인덱스\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "#하지만 손실값을 최적화 해서 모델을 만들어도 정확도는 낮음. 그 이유는 레이어가 하나라서\n",
    "#참고 reduce_xxx함수 http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221164644393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "심층 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구성하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다. \n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.8573486\n",
      "20 0.6263494\n",
      "30 0.4754187\n",
      "40 0.3670518\n",
      "50 0.2809546\n",
      "60 0.21329631\n",
      "70 0.15971468\n",
      "80 0.11947924\n",
      "90 0.090056084\n",
      "100 0.06901562\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3     4    5     6     7     8     9      10    11  \\\n",
      "0     1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.64  1.04   \n",
      "1     1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.38  1.05   \n",
      "2     1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.68  1.03   \n",
      "3     1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.80  0.86   \n",
      "4     1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.04   \n",
      "..   ..    ...   ...   ...   ...  ...   ...   ...   ...   ...    ...   ...   \n",
      "173   3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.70  0.64   \n",
      "174   3  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.70   \n",
      "175   3  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.20  0.59   \n",
      "176   3  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.30  0.60   \n",
      "177   3  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.61   \n",
      "\n",
      "       12    13  \n",
      "0    3.92  1065  \n",
      "1    3.40  1050  \n",
      "2    3.17  1185  \n",
      "3    3.45  1480  \n",
      "4    2.93   735  \n",
      "..    ...   ...  \n",
      "173  1.74   740  \n",
      "174  1.56   750  \n",
      "175  1.56   835  \n",
      "176  1.62   840  \n",
      "177  1.60   560  \n",
      "\n",
      "[178 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n",
    "                      'ml/machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "print(df_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deseo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n",
    "                     'ml/machine-learning-databases/wine/wine.data',\n",
    "                     header = None)\n",
    "\n",
    "Xw = df_wine.iloc[:,1:13].values\n",
    "yw = df_wine.iloc[:,0]\n",
    "\n",
    "Xw_train,Xw_test,yw_train,yw_test = train_test_split(Xw,yw, random_state=0)\n",
    "#print(yw)\n",
    "yw = yw.values.reshape(-1,1) \n",
    "#print(yw)\n",
    "type(yw)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(yw)\n",
    "oheyw = ohe.transform(yw).toarray()\n",
    "\n",
    "print(oheyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([12, 15], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다. \n",
    "W2 = tf.Variable(tf.random_uniform([15, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([15]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 6.079734\n",
      "20 6.007086\n",
      "30 3.8547165\n",
      "40 2.7519407\n",
      "50 2.2410362\n",
      "60 1.6841625\n",
      "70 1.40204\n",
      "80 1.179813\n",
      "90 1.0041957\n",
      "100 0.86447793\n",
      "110 0.75055015\n",
      "120 0.6556985\n",
      "130 0.57648957\n",
      "140 0.5114311\n",
      "150 0.46038944\n",
      "160 0.4226404\n",
      "170 0.39505133\n",
      "180 0.37358347\n",
      "190 0.35581258\n",
      "200 0.3406457\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(200):\n",
    "    sess.run(train_op, feed_dict={X: Xw, Y: oheyw})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: Xw, Y: oheyw}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 0 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 0 1 1 2 1 1 1 1 1 1 1 1 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "실제값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "정확도: 87.64\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: Xw}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: oheyw}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: Xw, Y: oheyw}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 계층을 더 늘린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([12, 15], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다. \n",
    "W2 = tf.Variable(tf.random_uniform([15, 10], -1., 1.))\n",
    "W3 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))         # 최종 12에 3으로 맞춰줌\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([15]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "b3 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "L2 = tf.add(tf.matmul(L1, W2), b2)\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L2, W3), b3)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.05)  # 0.01 에서 올린다\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 13.977132\n",
      "20 3.7374501\n",
      "30 2.379713\n",
      "40 1.4156941\n",
      "50 0.69697523\n",
      "60 0.6199104\n",
      "70 0.56100255\n",
      "80 0.524509\n",
      "90 0.4917484\n",
      "100 0.45978007\n",
      "110 0.43191227\n",
      "120 0.4072024\n",
      "130 0.38442072\n",
      "140 0.3617585\n",
      "150 0.34074032\n",
      "160 0.32000095\n",
      "170 0.30011258\n",
      "180 0.28090906\n",
      "190 0.26369777\n",
      "200 0.24878532\n",
      "210 0.23616914\n",
      "220 0.22465357\n",
      "230 0.21471544\n",
      "240 0.20594996\n",
      "250 0.19800772\n",
      "260 0.19087625\n",
      "270 0.18431161\n",
      "280 0.17793983\n",
      "290 0.17180727\n",
      "300 0.16549206\n",
      "310 0.15859945\n",
      "320 0.15130384\n",
      "330 0.14550279\n",
      "340 0.14017564\n",
      "350 0.1354856\n",
      "360 0.1315289\n",
      "370 0.1279926\n",
      "380 0.124820374\n",
      "390 0.12180894\n",
      "400 0.118934356\n",
      "410 0.116288505\n",
      "420 0.11391373\n",
      "430 0.11153394\n",
      "440 0.10911113\n",
      "450 0.106923446\n",
      "460 0.10473769\n",
      "470 0.102757014\n",
      "480 0.100828856\n",
      "490 0.09908329\n",
      "500 0.097883806\n",
      "510 0.09545784\n",
      "520 0.09356673\n",
      "530 0.091774896\n",
      "540 0.09011374\n",
      "550 0.08853555\n",
      "560 0.0866816\n",
      "570 0.085855894\n",
      "580 0.083772324\n",
      "590 0.092836134\n",
      "600 0.08291564\n",
      "610 0.085974865\n",
      "620 0.08041147\n",
      "630 0.08862614\n",
      "640 0.17550416\n",
      "650 0.5310673\n",
      "660 0.10762967\n",
      "670 0.08819784\n",
      "680 0.09229342\n",
      "690 0.08130684\n",
      "700 0.07489722\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(700):     # 200에서 700으로 올린다\n",
    "    sess.run(train_op, feed_dict={X: Xw, Y: oheyw})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: Xw, Y: oheyw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "실제값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "정확도: 97.75\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: Xw}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: oheyw}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: Xw, Y: oheyw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
